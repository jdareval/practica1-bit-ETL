{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e74433f",
   "metadata": {},
   "source": [
    "# --------TAREA 1 CONFIGURACION DE ENTORNO Y PROCESO ETL INICIOS ------\n",
    "\n",
    "\n",
    "**MAESTRIA EN CIBERSEGURIDAD**\n",
    "\n",
    "**INTELIGENCIA DE NEGOCIOS**\n",
    "\n",
    "**INTEGRANTES: JULIO AREVALO, EMILIO CORDOVA, NILSON ROMERO**\n",
    "\n",
    "**2025**\n",
    "\n",
    "## RESUMEN \n",
    "En este Notebook podrá encontrar el proceso completo de configuración de un entorno de análisis de datos y la ejecución de pipeline ETL. Se extraen datos de diferentes fuentes como PostgreSQL, CSV, JSON.\n",
    "Se procede a cargar data Warehouse en PostgreSQL y se realiza una explotación y segmentación inicial de los datos utilizando librerías como Pandas en Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d1740",
   "metadata": {},
   "source": [
    "# ----------IMPORTANCION DE LIBRERIAS CONEXION POSTGRESQL----------\n",
    "\n",
    "Se inicia la actividad estableciendo la conexión a la base de datos de destino, esta DB funcionara como un **Data Warehouse**.\n",
    "Utilizaremos como motor *DB* **PostgreSQL** esta base estará alojada en un contenedor **Docker**. Esto garantiza un entorno de desarrollo consistente, aislable y reproducible.\n",
    "Se definen los parámetros de conexión y se creará el *motor* *(engine)* utilizando las librería *SQLalchemy*. El motor actuará como puente entre nuestra herramienta de análisis en Python y la base de datos.\n",
    "En este proceso se ha implementado un bloque **try-except** para gestionar de forma precisa cualquier posible error durante el intento de conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- IMPORTACION DE LIBRERIAS -----\n",
    "# Se importan las librerías necesarias para la manipulación de datos y para la conexión con la base de datos \n",
    "# qué contendrá la data.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # crea la conexion a la base de datos\n",
    "import os # proporciona funciones para interactuar con el sistema operativo (manejar rutas de archivos)\n",
    "import json # para trabajar con datos en formato JSON\n",
    "from dotenv import load_dotenv # para cargar variables de entorno desde un archivo .env\n",
    "# ----- CONEXION A LA BASE DE DATOS POSTGRESQL EN DOCKER -----\n",
    "# --- 2. CARGAR Y LEER VARIABLES DE ENTORNO ---\n",
    "# La función load_dotenv() buscará y cargará las variables del archivo .env\n",
    "load_dotenv()\n",
    "print(\"Variables de entorno cargadas.\")\n",
    "\n",
    "# Leemos cada variable del entorno usando os.getenv()\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "#-----Creación del motor de conexión SQLalchemy-----\n",
    "#se utiliza un bloque try-except para el manejo de errores.\n",
    "#Si la conexión falla el programa informará del error en lugar de tenerse abruptamente.\n",
    "# --- 3. CONEXIÓN A LA BASE DE DATOS ---\n",
    "print(\"Iniciando conexión a la base de datos...\")\n",
    "try:\n",
    "    # La cadena de conexión ahora se construye con las variables cargadas, sin secretos en el código.\n",
    "    engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "    # Verificamos la conexión para asegurar que todo funcionó\n",
    "    with engine.connect() as connection:\n",
    "        print(\"¡Conexión a PostgreSQL exitosa!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"---> ERROR al conectar: {e}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c301d27",
   "metadata": {},
   "source": [
    "En la salida nos indica que el proceso de conexion ha sido exitoso \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9510bc",
   "metadata": {},
   "source": [
    "### CARGA MASIVA DE CSV A LA TABLA DE POSTGRESQL PROCESO ETL\n",
    "\n",
    "En esta sección del código se podrá encontrar el núcleo del proceso **ETL** para inyectar la base de datos **postgreSQL**.\n",
    "La consigna es consolidar múltiples archivos **CSV** relacionados en un único **data warehouse** y posteriormente se realizará el análisis de la información.\n",
    "\n",
    "Se describe los pasos:\n",
    "\n",
    "**Selección de la fuente EXTRACT:** Procedemos a definir una lista explícita de los archivos **CSV** que contienen las entidades del negocio las principales *(clientes, órdenes, productos)*\n",
    "\n",
    "**Bucle de procesamiento:** se itegra sobre cada archivo de la lista para procesarlo de manera automatizada.\n",
    "\n",
    "**Transformación ligera:** dentro del bucle se realiza una transformación básica para generar nombres de tabla limpios el lógicos a partir de los nombres de los archivos. ej. `olist_customers_dataset.csv` se convertirá en la tabla `customers`.\n",
    "\n",
    "**Carga:** se utiliza la función **to_sql** de *Pandas* para cargar los datos de cada archivo en su tabla correspondiente dentro del progress SQL. Se ha configurado para reemplazar la tabla si ya existe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96460b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta donde se encuentran los archivos CSV descargados\n",
    "\n",
    "data_path = r'C:\\Users\\j_dar\\downloads\\archive'\n",
    "\n",
    "# SELECCIONAMOS LOS ARCHIVOS CSV\n",
    "# Se crea una lista con los nombres de los archivos CSV que se van a cargar en la base de datos.\n",
    "files_to_load_in_db = [\n",
    "    'olist_customers_dataset.csv',\n",
    "    'olist_orders_dataset.csv',\n",
    "    'olist_order_items_dataset.csv',\n",
    "    'olist_products_dataset.csv',\n",
    "    'olist_sellers_dataset.csv'\n",
    "]\n",
    "\n",
    "#----- Blucle para cargar los archivos CSV en la base de datos -----\n",
    "# INICIAMOS EL PROCESO DE CARGA MASIVA DE LOS ARCHIVOS CSV A LA BASE DE DATOS\n",
    "\n",
    "print(\"\\nIniciando carga de datos a PostgreSQL...\")\n",
    "for file in files_to_load_in_db:\n",
    "    # El nombre de la tabla será el del archivo sin la extensión y prefijos\n",
    "    table_name = file.replace('olist_', '').replace('_dataset.csv', '')\n",
    "    df = pd.read_csv(os.path.join(data_path, file))\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    print(f\"Tabla '{table_name}' cargada con {len(df)} filas.\")\n",
    "print(\"Carga a PostgreSQL completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f1ba4",
   "metadata": {},
   "source": [
    "### GENERACION DE LOS DATAFRAME PRINCIPALES\n",
    "En este proceso procederemos a realizar la creacion de los dataframes con la libreria pandas.\n",
    "\n",
    "Se procede con la **extracción de datos desde tres fuentes distintas**, generando un **DataFrame** para cada una de ellas.\n",
    "Este proceso de simulación va a reflejar un escenario empresarial *real* dónde la información reside en un sistema.\n",
    "\n",
    "**DataFrame creados:**\n",
    "**`df_orders` (Desde PostgreSQL ):** Se consultará la tabla de nuestro **DataWarehouse** esta presenta la fuente de datos principales ya consolidada y estructura.\n",
    "**`df_order_reviews` (Desde CSV ):** Se leerá un archivo **CSV** externo, esto simula la ingesta de datos provenientes de un reporte externo o un sistema *Legacy*.\n",
    "**`df_json` (Desde JSON):** Se simula una fuente `JSON` el proceso consistirá en convertir primero un archivo `csv` de referencia a un archivo `JSON` Y luego leerlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Creacion del dataframe 1 desde PostGreSQL-----\n",
    "\n",
    "print(\"1. Creando dataframe desde PostgreSQL tabla 'orders'\")\n",
    "query_orders = \"SELECT * FROM orders;\"\n",
    "df_orders = pd.read_sql(query_orders, engine)\n",
    "\n",
    "# ----- Creacion del dataframe 2 desde CSV -----\n",
    "\n",
    "print(\"2. CREACION DE DATAFRAME DESDE CSV ('ORDER_REVIWS') \")\n",
    "df_order_reviews = pd.read_csv(os.path.join(data_path, 'olist_order_reviews_dataset.csv'))\n",
    "\n",
    "#----- creacion del dataframe 3 desde JSON -----\n",
    "\n",
    "print(\"3. CREACION DE DATAFRAME DESDE JSON ('product_category_translation') \")\n",
    "\n",
    "# convertir el csv a un archivo JSON PARA SIMULAR LA FUENTE\n",
    "df_temp_json = pd.read_csv(os.path.join(data_path, 'product_category_name_translation.csv'))\n",
    "json_path = os.path.join(data_path, 'categories.json')\n",
    "df_temp_json.to_json(json_path, orient='records', lines=True)\n",
    "\n",
    "# leer el archivo JSON\n",
    "df_json = pd.read_json(json_path, lines=True)\n",
    "\n",
    "print(\"LOS DATAFRAME PRINCIPALES HAN SIDO CREADOS EXITOSAMENTE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034888c",
   "metadata": {},
   "source": [
    "# EXPLORACION Y FILTROS DEL DATAFRAME  DE POSTGRESQL DF_POSTGRES\n",
    "\n",
    "Una vez que se ha procedido a cargar los **DataFrame** el siguiente paso es realizar el ** análisis exploratorio de datos EDA** este paso es fundamental para comprender la naturaleza de nuestra información antes de realizar otro tipo de análisis que sean más complejos.\n",
    "\n",
    "Se inicia con él **DataFrame** `df_orders`, Que contiene la información de las órdenes de compra en nuestra base de datos **PostgreSQL**\n",
    "\n",
    "**Se ha dividido el análisis en dos partes:**\n",
    "\n",
    "**Exploración inicial:** Se utilizarán los métodos `.info()` y `.head()` de `Pandas` para responder \n",
    "\n",
    "**Filtrado y Segmentación:** Se aplicarán tres filtros específicos para segmentar los datos y responder a preguntas de negocio iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa80c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploracion de los dataframes creados\n",
    "# Exploramos los dataframes creados para entender su estructura y contenido.\n",
    "\n",
    "print(\"exploracion del postgreSQL df_postgres (orders)\")\n",
    "print(df_orders.info())\n",
    "print(\"\\nPrimeras cinco filas:\")\n",
    "display(df_orders.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTRO PEDIDOS ENTREGADOS\n",
    "entregados = df_orders[df_orders['order_status'] == 'delivered']\n",
    "print(f\"HAY {len(entregados)} PEDIDOS ENTREGADOS\")\n",
    "\n",
    "# PEDIDOS REALIZADOS EN EL 2017\n",
    "pedidos_2017 = df_orders[pd.to_datetime(df_orders['order_purchase_timestamp']).dt.year == 2017]\n",
    "print(f\"hay {len(pedidos_2017)} pedidos realizados en el 2017\")\n",
    "pedidos_2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PEDIDOS CANCELADOS O NO DISPONIBLES -----\n",
    "cancelados = df_orders[df_orders['order_status'].isin(['canceled', 'unavailable'])]\n",
    "print(f\"HAY {len(cancelados)} PEDIDOS CANCELADOS O NO DISPONIBLES\")\n",
    "cancelados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d600a1e",
   "metadata": {},
   "source": [
    "### EXPLORACION Y FILTROS DATAFRAME CSV\n",
    "\n",
    "Esta vez enfocándonos en el DataFrame `df_order_reviews`, que fue cargado desde nuestra fuente CSV obligatoria.\n",
    "\n",
    "Estos datos son **crítico para entender la satisfacción del cliente**, ya que contiene las puntuaciones y comentarios que los usuarios han dejado después de una compra.\n",
    "\n",
    "\n",
    "\n",
    "**El proceso es el sigguiente:**\n",
    "\n",
    "**Exploración de la Estructura de Datos** \n",
    "\n",
    "**Interpretación de Hallazgos Preliminares**\n",
    "\n",
    "Se analizará la salida de la exploración para identificar puntos clave, como la cantidad de comentarios escritos versus reseñas sin texto, y la necesidad de convertir tipos de datos para análisis futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- exploracion de df_csv (reviews) -----\")\n",
    "print(df_order_reviews.info())\n",
    "print(\"\\nPrimeras cinco filas:\")\n",
    "display(df_order_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- reseñas con una puntuación de cinco estrellas -----\n",
    "resenias_5_estrellas = df_order_reviews[df_order_reviews['review_score'] == 5]\n",
    "print(f\"HAY {len(resenias_5_estrellas)} RESEÑAS CON 5 ESTRELLAS\")\n",
    "resenias_5_estrellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65001065",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- RESEÑAS CON COMENTARIOS -----\n",
    "resenias_con_comentarios = df_order_reviews[df_order_reviews['review_comment_message'].notnull() & (df_order_reviews['review_comment_message'].str.strip() != '')]\n",
    "print(f\"HAY {len(resenias_con_comentarios)} RESEÑAS CON COMENTARIOS\")\n",
    "resenias_con_comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b92522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESEÑAS CON LA MINIMAPUNTUACIÓN 1 ESTRELLA\n",
    "resenias_1_estrella = df_order_reviews[df_order_reviews['review_score'] == 1]\n",
    "print(f\"HAY {len(resenias_1_estrella)} RESEÑAS CON 1 ESTRELLA\")\n",
    "resenias_1_estrella"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81325d",
   "metadata": {},
   "source": [
    "### ANALISIS DE JSON \n",
    "Finalmente, se realiza el Análisis Exploratorio de Datos (EDA) para el DataFrame `df_json`. Este conjunto de datos fue cargado desde la **fuente JSON simulada** y contiene la traducción de los nombres de las categorías de productos del portugués al inglés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORACIÓN DE DF_JSON \n",
    "print(\"----- Exploración de df_json (categories_translation) -----\")\n",
    "print(df_json.info())\n",
    "print(\"\\nPrimeras cinco filas:\")\n",
    "display(df_json.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CATEGORIAS RELACIONADAS CON LA CAMA Y EL BAÑO -----\n",
    "cama_bano = df_json[df_json['product_category_name_english'] == 'bed_bath_table']\n",
    "print(f\"HAY {len(cama_bano)} CATEGORIAS RELACIONADAS CON LA CAMA Y EL BAÑO\")\n",
    "cama_bano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORIAS RELACIONADAS CON LA PALABRA COMPUTADORA EN PORTUGUES\n",
    "computadoras_pt = df_json[df_json['product_category_name'].str.contains('informatica', na=False)]\n",
    "print(f\"HAY {len(computadoras_pt)} CATEGORIAS RELACIONADAS CON LA PALABRA COMPUTADORA EN PORTUGUES\")\n",
    "computadoras_pt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORIA REALCIONADA CON EL DEPORTE Y OCIO\n",
    "deporte_ocio = df_json[df_json['product_category_name_english'] == 'sports_leisure']\n",
    "print(f\"HAY {len(deporte_ocio)} CATEGORIAS RELACIONADAS CON EL DEPORTE Y OCIO\")\n",
    "deporte_ocio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
